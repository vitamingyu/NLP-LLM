{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAfDWiyntgP9f6aje3lSEU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitamingyu/NLP-LLM/blob/main/tf_48Transformer_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rcghY70nFTEG"
      },
      "outputs": [],
      "source": [
        "# imdb dataset으로 감성 분류\n",
        "# Transformer는 입력 데이터들간의 상호작용을 고려하는 self-attention\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder block\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)  # sequence에 따른 고정 길이 정규화로 batch normalization에 비해 언어 모델\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward NN\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n",
        "\n",
        "def build_sentment_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    embedding_layer=layers.Embedding(input_dim=1000, output_dim=64, input_length=input_shape[0])(inputs)\n",
        "    x = embedding_layer + tf.random.normal(shape=tf.shape(embedding_layer))\n",
        "    # Transformer blocks\n",
        "    for _ in range(num_transformer_blocks):\n",
        "      x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # 공간차원 축소\n",
        "\n",
        "    #MLP\n",
        "    for dim in mlp_units:\n",
        "      x = layers.Dense(dim, activation = 'relu')(x)\n",
        "      x = layers.Dropout(mlp_dropout)(x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return keras.Model(inputs = inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "iltSKvJGJShe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train),(x_val, y_val) = keras.datasets.imdb.load_data(num_words=10000)\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=100)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=100)\n",
        "\n",
        "# example usage\n",
        "input_shape=(100,)\n",
        "head_size = 256\n",
        "num_heads = 4\n",
        "ff_dim = 4\n",
        "num_transformer_blocks = 4\n",
        "mlp_units = [128]\n",
        "dropout = 0.25\n",
        "mlp_dropout=0.25\n",
        "\n",
        "model = build_sentment_model(\n",
        "    input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout, mlp_dropout\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZq300TvJTzE",
        "outputId": "597df258-84ef-47df-95f0-b40115be27bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)     (None, 100, 64)              64000     ['input_9[0][0]']             \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape_8 (TFOp  (3,)                         0         ['embedding_8[0][0]']         \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.random.normal_8 (TFOpLa  (None, 100, 64)              0         ['tf.compat.v1.shape_8[0][0]']\n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_48 (T  (None, 100, 64)              0         ['embedding_8[0][0]',         \n",
            " FOpLambda)                                                          'tf.random.normal_8[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization_41 (La  (None, 100, 64)              128       ['tf.__operators__.add_48[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_20 (M  (None, 100, 64)              265280    ['layer_normalization_41[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_41[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_49 (T  (None, 100, 64)              0         ['multi_head_attention_20[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'tf.__operators__.add_48[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_42 (La  (None, 100, 64)              128       ['tf.__operators__.add_49[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)          (None, 100, 4)               260       ['layer_normalization_42[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)        (None, 100, 4)               0         ['conv1d_40[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)          (None, 100, 64)              320       ['dropout_25[0][0]']          \n",
            "                                                                                                  \n",
            " tf.__operators__.add_50 (T  (None, 100, 64)              0         ['conv1d_41[0][0]',           \n",
            " FOpLambda)                                                          'tf.__operators__.add_49[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_43 (La  (None, 100, 64)              128       ['tf.__operators__.add_50[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_21 (M  (None, 100, 64)              265280    ['layer_normalization_43[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_43[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_51 (T  (None, 100, 64)              0         ['multi_head_attention_21[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'tf.__operators__.add_50[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_44 (La  (None, 100, 64)              128       ['tf.__operators__.add_51[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)          (None, 100, 4)               260       ['layer_normalization_44[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)        (None, 100, 4)               0         ['conv1d_42[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)          (None, 100, 64)              320       ['dropout_26[0][0]']          \n",
            "                                                                                                  \n",
            " tf.__operators__.add_52 (T  (None, 100, 64)              0         ['conv1d_43[0][0]',           \n",
            " FOpLambda)                                                          'tf.__operators__.add_51[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_45 (La  (None, 100, 64)              128       ['tf.__operators__.add_52[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_22 (M  (None, 100, 64)              265280    ['layer_normalization_45[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_45[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_53 (T  (None, 100, 64)              0         ['multi_head_attention_22[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'tf.__operators__.add_52[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_46 (La  (None, 100, 64)              128       ['tf.__operators__.add_53[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)          (None, 100, 4)               260       ['layer_normalization_46[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)        (None, 100, 4)               0         ['conv1d_44[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)          (None, 100, 64)              320       ['dropout_27[0][0]']          \n",
            "                                                                                                  \n",
            " tf.__operators__.add_54 (T  (None, 100, 64)              0         ['conv1d_45[0][0]',           \n",
            " FOpLambda)                                                          'tf.__operators__.add_53[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_47 (La  (None, 100, 64)              128       ['tf.__operators__.add_54[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_23 (M  (None, 100, 64)              265280    ['layer_normalization_47[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_47[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_55 (T  (None, 100, 64)              0         ['multi_head_attention_23[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'tf.__operators__.add_54[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_48 (La  (None, 100, 64)              128       ['tf.__operators__.add_55[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)          (None, 100, 4)               260       ['layer_normalization_48[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)        (None, 100, 4)               0         ['conv1d_46[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)          (None, 100, 64)              320       ['dropout_28[0][0]']          \n",
            "                                                                                                  \n",
            " tf.__operators__.add_56 (T  (None, 100, 64)              0         ['conv1d_47[0][0]',           \n",
            " FOpLambda)                                                          'tf.__operators__.add_55[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_5  (None, 64)                   0         ['tf.__operators__.add_56[0][0\n",
            "  (GlobalAveragePooling1D)                                          ]']                           \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 128)                  8320      ['global_average_pooling1d_5[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)        (None, 128)                  0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 1)                    129       ['dropout_29[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1136913 (4.34 MB)\n",
            "Trainable params: 1136913 (4.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xit2LvMBKhN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}