{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOuF6eH1VLtWMeEYoEEse51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitamingyu/NLP-LLM/blob/main/tf_48Transformer_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rcghY70nFTEG"
      },
      "outputs": [],
      "source": [
        "# imdb dataset으로 감성 분류\n",
        "# Transformer는 입력 데이터들간의 상호작용을 고려하는 self-attention\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder block\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)  # sequence에 따른 고정 길이 정규화로 batch normalization에 비해 언어 모델\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward NN\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n",
        "\n",
        "def build_sentment_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    embedding_layer=layers.Embedding(input_dim=1000, output_dim=64, input_length=input_shape[0])(inputs)\n",
        "    x = embedding_layer + tf.random.normal(shape=tf.shape(embedding_layer))\n",
        "    # Transformer blocks\n",
        "    for _ in range(num_transformer_blocks):\n",
        "      x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # 공간차원 축소\n",
        "\n",
        "    #MLP\n",
        "    for dim in mlp_units:\n",
        "      x = layers.Dense(dim, activation = 'relu')(x)\n",
        "      x = layers.Dropout(mlp_dropout)(x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return keras.Model(inputs = inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "iltSKvJGJShe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train),(x_val, y_val) = keras.datasets.imdb.load_data(num_words=10000)\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=100)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=100)\n",
        "\n",
        "# example usage\n",
        "input_shape=(100,)\n",
        "head_size = 256\n",
        "num_heads = 4\n",
        "ff_dim = 4\n",
        "num_transformer_blocks = 4\n",
        "mlp_units = [128]\n",
        "dropout = 0.25\n",
        "mlp_dropout=0.25\n",
        "\n",
        "model = build_sentment_model(\n",
        "    input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout, mlp_dropout\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZq300TvJTzE",
        "outputId": "0c34ccc7-6687-4eda-b8da-bf4172ccd704"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 100, 64)              64000     ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape (TFOpLa  (3,)                         0         ['embedding[0][0]']           \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.random.normal (TFOpLamb  (None, 100, 64)              0         ['tf.compat.v1.shape[0][0]']  \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 100, 64)              0         ['embedding[0][0]',           \n",
            " Lambda)                                                             'tf.random.normal[0][0]']    \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 100, 64)              128       ['tf.__operators__.add[0][0]']\n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 100, 64)              265280    ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None, 100, 64)              0         ['multi_head_attention[0][0]',\n",
            " OpLambda)                                                           'tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_1[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 100, 4)               260       ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 100, 4)               0         ['conv1d[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 100, 64)              320       ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TF  (None, 100, 64)              0         ['conv1d_1[0][0]',            \n",
            " OpLambda)                                                           'tf.__operators__.add_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_2[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 100, 64)              265280    ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TF  (None, 100, 64)              0         ['multi_head_attention_1[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'tf.__operators__.add_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_3[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 100, 4)               260       ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 100, 4)               0         ['conv1d_2[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 100, 64)              320       ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TF  (None, 100, 64)              0         ['conv1d_3[0][0]',            \n",
            " OpLambda)                                                           'tf.__operators__.add_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_4[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 100, 64)              265280    ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TF  (None, 100, 64)              0         ['multi_head_attention_2[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'tf.__operators__.add_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_5[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 100, 4)               260       ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 100, 4)               0         ['conv1d_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 100, 64)              320       ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TF  (None, 100, 64)              0         ['conv1d_5[0][0]',            \n",
            " OpLambda)                                                           'tf.__operators__.add_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_6[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 100, 64)              265280    ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TF  (None, 100, 64)              0         ['multi_head_attention_3[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'tf.__operators__.add_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_7[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 100, 4)               260       ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 100, 4)               0         ['conv1d_6[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 100, 64)              320       ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TF  (None, 100, 64)              0         ['conv1d_7[0][0]',            \n",
            " OpLambda)                                                           'tf.__operators__.add_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d (  (None, 64)                   0         ['tf.__operators__.add_8[0][0]\n",
            " GlobalAveragePooling1D)                                            ']                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  8320      ['global_average_pooling1d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 128)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1)                    129       ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1136913 (4.34 MB)\n",
            "Trainable params: 1136913 (4.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-4),metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs = 10, batch_size=32, verbose=2)\n",
        "print(history.history)"
      ],
      "metadata": {
        "id": "xit2LvMBKhN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2aadcac-4a90-47ae-ff94-eadb39c87dc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 - 91s - loss: 0.6955 - acc: 0.5031 - val_loss: 0.6934 - val_acc: 0.5077 - 91s/epoch - 116ms/step\n",
            "Epoch 2/10\n",
            "782/782 - 40s - loss: 0.6893 - acc: 0.5355 - val_loss: 0.6782 - val_acc: 0.5706 - 40s/epoch - 52ms/step\n",
            "Epoch 3/10\n",
            "782/782 - 38s - loss: 0.6658 - acc: 0.5963 - val_loss: 0.6488 - val_acc: 0.6202 - 38s/epoch - 48ms/step\n",
            "Epoch 4/10\n",
            "782/782 - 37s - loss: 0.6390 - acc: 0.6352 - val_loss: 0.6256 - val_acc: 0.6462 - 37s/epoch - 48ms/step\n",
            "Epoch 5/10\n",
            "782/782 - 54s - loss: 0.6154 - acc: 0.6654 - val_loss: 0.6063 - val_acc: 0.6688 - 54s/epoch - 69ms/step\n",
            "Epoch 6/10\n",
            "782/782 - 36s - loss: 0.5960 - acc: 0.6814 - val_loss: 0.5866 - val_acc: 0.6856 - 36s/epoch - 46ms/step\n",
            "Epoch 7/10\n",
            "782/782 - 36s - loss: 0.5765 - acc: 0.6980 - val_loss: 0.5743 - val_acc: 0.6952 - 36s/epoch - 46ms/step\n",
            "Epoch 8/10\n",
            "782/782 - 38s - loss: 0.5634 - acc: 0.7085 - val_loss: 0.5522 - val_acc: 0.7149 - 38s/epoch - 49ms/step\n",
            "Epoch 9/10\n",
            "782/782 - 41s - loss: 0.5413 - acc: 0.7236 - val_loss: 0.5392 - val_acc: 0.7265 - 41s/epoch - 52ms/step\n",
            "Epoch 10/10\n",
            "782/782 - 36s - loss: 0.5295 - acc: 0.7378 - val_loss: 0.5271 - val_acc: 0.7349 - 36s/epoch - 46ms/step\n",
            "{'loss': [0.6954618692398071, 0.6892575025558472, 0.6657510995864868, 0.6390061974525452, 0.6153548955917358, 0.5959640741348267, 0.5765230059623718, 0.5634006261825562, 0.5412522554397583, 0.5294881463050842], 'acc': [0.5030800104141235, 0.5354800224304199, 0.5963199734687805, 0.6351600289344788, 0.6654000282287598, 0.6813600063323975, 0.6980000138282776, 0.7085199952125549, 0.7236400246620178, 0.7378399968147278], 'val_loss': [0.6934052109718323, 0.6782410740852356, 0.6487886309623718, 0.6255708336830139, 0.6063196063041687, 0.5865598917007446, 0.5743241310119629, 0.5521518588066101, 0.5391566157341003, 0.5270694494247437], 'val_acc': [0.5077199935913086, 0.5705999732017517, 0.620199978351593, 0.6462000012397766, 0.6687999963760376, 0.6855999827384949, 0.6951599717140198, 0.7149199843406677, 0.7264800071716309, 0.7348799705505371]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = ['I love movie', \"This movie sucks, it's so boring\"]\n",
        "max_len = 100\n",
        "tok = keras.datasets.imdb.get_word_index()\n",
        "text_sequence = [[tok[word] if word in tok else 0 for word in sample.split()] for sample in text_sample]\n",
        "text_sequence = keras.preprocessing.sequence.pad_sequences(text_sequence, maxlen=max_len)\n",
        "print(text_sequence)\n",
        "\n",
        "pred = model.predict(text_sequence)\n",
        "\n",
        "b_pred = (pred > 0.5 ).astype(int)\n",
        "\n",
        "for i, samp in enumerate(text_sample):\n",
        "  print(f'sample : {samp}')\n",
        "  print(f'predicted : {\"Positive\" if b_pred[i] == 1 else \"Negative\"}')\n",
        "  print(f\"confidencs : {pred[i][0]:.3f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNy3Bpj2vsCT",
        "outputId": "f674c6ee-de49-458b-82ed-d251adde8bb4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0 116  17]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0  17   0  42  35 354]]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "sample : I love movie\n",
            "predicted : Positive\n",
            "confidencs : 0.652\n",
            "\n",
            "sample : This movie sucks, it's so boring\n",
            "predicted : Negative\n",
            "confidencs : 0.455\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiY1ApnVznRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}