{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeSooXTQtTqhseqZceF1uK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitamingyu/NLP-LLM/blob/main/tf_43attension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gsCoO8i7nNhT"
      },
      "outputs": [],
      "source": [
        "# 어텐션 : 디코더에서 출력단어를 예측하는데 매 시점마다 인코더에서 전체 입력문장을 다시 참조하는 방식\n",
        "# 해당시점에서 예측해야 할 단어와 연관이 있는 입력 단어에 좀 더 집중해서 작업한다.\n",
        "# seq2seq 알고리즘에 문제점 중 일부를 개선\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, LSTM, Dense, Concatenate, Attention\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가상의 파라미터에 대한 초기값\n",
        "input_length = 10\n",
        "output_length = 10\n",
        "vocab_size = 100\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "# encoder정의\n",
        "encoder_inputs = Input(shape=(input_length, embedding_dim))\n",
        "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "encoder_outputs,_,_ = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# decoder정의\n",
        "decoder_inputs = Input(shape=(output_length, embedding_dim))\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True)\n",
        "decoder_outputs = decoder_lstm(decoder_inputs)\n",
        "\n",
        "# Attension 레이어\n",
        "attention_layer = Attention()\n",
        "attention_output = attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Attention 레이어는 디코더의 출력과 인코더의 출력 사이에 관계를 계산하여 중요 정보에 집중할 수 있도록 도움을 준다\n",
        "concat_layer = Concatenate(axis= -1)\n",
        "decoder_concat_input = concat_layer([decoder_outputs, attention_output])\n",
        "\n",
        "# 출력 레이어 : 최종적으로 Dense를 통해 예측을 수행한다\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFzb1fBDodob",
        "outputId": "a551cef8-50cc-429d-9435-a1b0b85ec7ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)       [(None, 10, 64)]             0         []                            \n",
            "                                                                                                  \n",
            " input_11 (InputLayer)       [(None, 10, 64)]             0         []                            \n",
            "                                                                                                  \n",
            " lstm_11 (LSTM)              (None, 10, 128)              98816     ['input_12[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_10 (LSTM)              [(None, 10, 128),            98816     ['input_11[0][0]']            \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 10, 128)              0         ['lstm_11[0][0]',             \n",
            "                                                                     'lstm_10[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 10, 256)              0         ['lstm_11[0][0]',             \n",
            "                                                                     'attention[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 10, 100)              25700     ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 223332 (872.39 KB)\n",
            "Trainable params: 223332 (872.39 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDIiSFE7sS2R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}