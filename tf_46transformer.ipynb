{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzqBu/aSUY2lqlaNG0GoV1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitamingyu/NLP-LLM/blob/main/tf_46transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xJmwP7rAvfyN"
      },
      "outputs": [],
      "source": [
        "# Transformer : RNN을 제거하고 Attention으로만 구성도니 서로 다른 인코더와 디코더가 여러 개 쌓인 형태의 네트워크\n",
        "# 인코더 또는 디코더 안에는 self attention과 feedford 신경망(Dense)으로 구성 되어 있다.\n",
        "# self attention 함수는 주어진 Query에 대해서 key와 유사도를 구한다. 그리고 이 유사도를 키와 매핑되어 있는 값(value)에 반영한다.\n",
        "# self attention은 RNN처럼 순서대로 처리 하는 방식이 아니라 해당하는 단어와 관련된 뜻을 찾기 위한 어텐션을 말한다\n",
        "# 간단한 구조 이해 \\\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "vocab_size = 20000\n",
        "maxlen = 200\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), len(x_val))\n",
        "\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "print(x_train[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AziNY8Lsx3gf",
        "outputId": "4a04388a-a7a3-4313-afa0-11b374a2a237"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 25000\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    1   14   47    8   30   31    7    4  249  108    7\n",
            "    4 5974   54   61  369   13   71  149   14   22  112    4 2401  311\n",
            "   12   16 3711   33   75   43 1829  296    4   86  320   35  534   19\n",
            "  263 4821 1301    4 1873   33   89   78   12   66   16    4  360    7\n",
            "    4   58  316  334   11    4 1716   43  645  662    8  257   85 1200\n",
            "   42 1228 2578   83   68 3912   15   36  165 1539  278   36   69    2\n",
            "  780    8  106   14 6905 1338   18    6   22   12  215   28  610   40\n",
            "    6   87  326   23 2300   21   23   22   12  272   40   57   31   11\n",
            "    4   22   47    6 2307   51    9  170   23  595  116  595 1352   13\n",
            "  191   79  638   89    2   14    9    8  106  607  624   35  534    6\n",
            "  227    7  129  113]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token and Position embedding\n",
        "inputs = layers.Input(shape=(200,))\n",
        "x = layers.Embedding(input_dim=128, output_dim=32)(inputs)\n",
        "print(tf.keras.Model(inputs=inputs, outputs=x).summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvZNsRLkylQc",
        "outputId": "6498b616-8bd0-49ee-972e-9312d42ab972"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 32)           4096      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4096 (16.00 KB)\n",
            "Trainable params: 4096 (16.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(200,))\n",
        "x = layers.Embedding(input_dim=128, output_dim=32)(inputs)\n",
        "positions = tf.range(start=0, limit=200, delta=1)  # 각 단어 위치에 따른 index값을 embedding하여 feature를 추출한 후 합을 구함\n",
        "positions = layers.Embedding(input_dim=200, output_dim=32)(positions)\n",
        "embedding_layer = x + positions\n",
        "\n",
        "# Multi-head attention을 위한 간단한 구조 작성\n",
        "EmbeddingDim=5\n",
        "WordLen = 5\n",
        "\n",
        "inputs = layers.Input(shape=(WordLen,))\n",
        "positions = tf.range(start=0, limit=WordLen, delta=1)\n",
        "positions = layers.Embedding(input_dim=WordLen, output_dim=EmbeddingDim)(positions)\n",
        "x = layers.Embedding(input_dim=200, output_dim=EmbeddingDim)(inputs)\n",
        "embedding_layer = x + positions\n",
        "\n",
        "# num_heads=1: 현재 Attention Matrix를 만드는 작업을 몇번할 것인가를 결정\n",
        "attention_output = layers.MultiHeadAttention(num_heads=1, key_dim=1, use_bias=False)(embedding_layer, embedding_layer)\n",
        "print(tf.keras.Model(inputs=inputs, outputs=attention_output).summary())\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=attention_output)\n",
        "print([w.shape for w in model.get_weights()])\n",
        "# [(200, 5),                    (5, 1, 1), (5, 1, 1), (5, 1, 1),                                  (1, 1, 5)]\n",
        "# 위는 embedding_layer weight,   Query         key      Value에 대한 weight       dot production내부에서 transpose되어 순서가 변경\n",
        "\n",
        "print('------------------------')\n",
        "# MultiGeadAttention layer에 num_heads=2, key_dim=3으 변경해 보자\n",
        "inputs = layers.Input(shape=(WordLen,))\n",
        "positions = tf.range(start=0, limit=WordLen, delta=1)\n",
        "positions = layers.Embedding(input_dim=WordLen, output_dim=EmbeddingDim)(positions)\n",
        "x = layers.Embedding(input_dim=200, output_dim=EmbeddingDim)(inputs)\n",
        "embedding_layer = x + positions\n",
        "\n",
        "# num_heads=1: 현재 Attention Matrix를 만드는 작업을 몇번할 것인가를 결정\n",
        "# attention_output = layers.MultiHeadAttention(num_heads=2, key_dim=3, use_bias=False)(embedding_layer, embedding_layer)\n",
        "attention_output = layers.MultiHeadAttention(num_heads=2, key_dim=3, use_bias=True)(embedding_layer, embedding_layer)\n",
        "print(tf.keras.Model(inputs=inputs, outputs=attention_output).summary())\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=attention_output)\n",
        "print([w.shape for w in model.get_weights()])\n",
        "# [(200, 5), (5, 2, 3), (5, 2, 3), (5, 2, 3), (2, 3, 5)]\n",
        "# [(200, 5), (5, 2, 3), (2, 3), (5, 2, 3), (2, 3), (5, 2, 3), (2, 3), (2, 3, 5), (5,)] use_bias=True하면 중간에 상수가 더해짐\n",
        "# 파라미터에 대한 존재 이유, 연산 등의 이해가 있다면, 다음으로 Scaled dot production Attention만 이해하면 Transformer에 기초\n",
        "# 단어들의 유사도와 같은 역할을 하는 Attention score를 구하고 이를 Value에 곱해주는 작업을 Single-Head Attention이 한다\n",
        "# Single-Head Attention을 여러개 병렬로 처리하면 이게 바로 MultiHeadAttention이 된다\n",
        "# 이때 num_heads는 이Attention을 만드는 수행을 몇번할것인가를"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPIYXLvt0Yac",
        "outputId": "e9577024-16ec-4a81-ecaa-2b7ec3a3eb0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_17 (InputLayer)       [(None, 5)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding_31 (Embedding)    (None, 5, 5)                 1000      ['input_17[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (T  (None, 5, 5)                 0         ['embedding_31[0][0]']        \n",
            " FOpLambda)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (Mu  (None, 5, 5)                 20        ['tf.__operators__.add_13[0][0\n",
            " ltiHeadAttention)                                                  ]',                           \n",
            "                                                                     'tf.__operators__.add_13[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1020 (3.98 KB)\n",
            "Trainable params: 1020 (3.98 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[(200, 5), (5, 1, 1), (5, 1, 1), (5, 1, 1), (1, 1, 5)]\n",
            "------------------------\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_18 (InputLayer)       [(None, 5)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding_33 (Embedding)    (None, 5, 5)                 1000      ['input_18[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (T  (None, 5, 5)                 0         ['embedding_33[0][0]']        \n",
            " FOpLambda)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (Mu  (None, 5, 5)                 143       ['tf.__operators__.add_14[0][0\n",
            " ltiHeadAttention)                                                  ]',                           \n",
            "                                                                     'tf.__operators__.add_14[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1143 (4.46 KB)\n",
            "Trainable params: 1143 (4.46 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[(200, 5), (5, 2, 3), (2, 3), (5, 2, 3), (2, 3), (5, 2, 3), (2, 3), (2, 3, 5), (5,)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 MultiHeadAttention을 통과한 분류 모델을 작성\n",
        "EmbeddingDim = 128\n",
        "WordLen = 200\n",
        "\n",
        "inputs = layers.Input(shape=(WordLen,))\n",
        "positions = tf.range(start=0, limit=WordLen, delta=1)\n",
        "positions = layers.Embedding(input_dim=WordLen, output_dim=EmbeddingDim)(positions)\n",
        "x = layers.Embedding(input_dim=200, output_dim=EmbeddingDim)(inputs)\n",
        "embedding_layer = x + positions\n",
        "\n",
        "attention_output = layers.MultiHeadAttention(num_heads=1, key_dim=2, use_bias=True)(embedding_layer, embedding_layer)\n",
        "x = layers.GlobalAveragePooling1D()(attention_output)\n",
        "# MultiHeadAttention을 통과한 output은 Embedding layer의 output과 동일하므로 GlobalAveragePooling1D을 통해 Vector화 할 수 있다\n",
        "outputs=layers.Dense(2, activation='softmax')(x)\n",
        "print(tf.keras.Model(inputs=inputs, outputs=attention_output).summary())\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcAeGADB1LxT",
        "outputId": "81c1649b-d5d7-487a-d5e3-9276b75a0bdc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_21\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_20 (InputLayer)       [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_37 (Embedding)    (None, 200, 128)             25600     ['input_20[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (T  (None, 200, 128)             0         ['embedding_37[0][0]']        \n",
            " FOpLambda)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (M  (None, 200, 128)             1158      ['tf.__operators__.add_16[0][0\n",
            " ultiHeadAttention)                                                 ]',                           \n",
            "                                                                     'tf.__operators__.add_16[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 26758 (104.52 KB)\n",
            "Trainable params: 26758 (104.52 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_20 (InputLayer)       [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_37 (Embedding)    (None, 200, 128)             25600     ['input_20[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (T  (None, 200, 128)             0         ['embedding_37[0][0]']        \n",
            " FOpLambda)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (M  (None, 200, 128)             1158      ['tf.__operators__.add_16[0][0\n",
            " ultiHeadAttention)                                                 ]',                           \n",
            "                                                                     'tf.__operators__.add_16[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 128)                  0         ['multi_head_attention_10[0][0\n",
            "  (GlobalAveragePooling1D)                                          ]']                           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 2)                    258       ['global_average_pooling1d_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 27016 (105.53 KB)\n",
            "Trainable params: 27016 (105.53 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}